{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6bb30bb-a864-41d6-8553-1d48f293b082",
   "metadata": {},
   "source": [
    "# Experiment 2: ViT-Lite-7/4 for Small Scale Writer Recognition\n",
    "Note: All (hyper)parameters according to [1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3244548-1559-405a-9fc5-8fe57e0013de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from timm.loss.cross_entropy import LabelSmoothingCrossEntropy\n",
    "\n",
    "# add parent directory of this report to path, in order to import files from the `src` folder\n",
    "pardir = os.path.join(os.getcwd(), os.pardir)\n",
    "if pardir not in sys.path:\n",
    "    sys.path.append(pardir)\n",
    "\n",
    "from src.datasets import CVLCroppedDataset\n",
    "from src.lr_schedulers import WarmUpLR\n",
    "from src.model_variants import vit_lite_7_4\n",
    "from src.preprocessing import TransformationPipeline, SIFTPatchExtractor, OtsuBinarization\n",
    "from src.pytorch_utils import seed_worker, set_all_seeds, Trainer, ClassificationTester, RetrievalTester\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d339aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 417 # generated with random.org (range 0 to 2^16)\n",
    "EXPERIMENT_NAME = f\"experiment-2_writer-recognition_seed-{SEED}\"\n",
    "\n",
    "LOG_DIR = os.path.join(os.curdir, \"runs\")\n",
    "SAVED_MODELS_DIR = os.path.join(os.curdir, \"saved_models\")\n",
    "DATA_DIR = os.path.join(os.pardir, \"data\")\n",
    "PREPROCESSED_FILES_DIRNAME = \"cvl-1-1_with-enrollment_experiment_pages\"\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 3e-2\n",
    "NUM_EPOCHS_WARMUP = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_CLASSES = 50\n",
    "IMG_SIZE = 32\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "DIM_SECOND_LAST_LAYER = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db566ae4",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "If not already done, we download and preprocess the desired dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_pipeline = TransformationPipeline(\n",
    "    os.path.join(os.pardir, \"dataset_splits\", \"cvl-1-1_with-enrollment_experiment_pages.csv\"), \n",
    "    SIFTPatchExtractor(sigma=3.75),\n",
    "    PREPROCESSED_FILES_DIRNAME, \n",
    "    pipeline_items=[OtsuBinarization()]\n",
    ")\n",
    "\n",
    "cvl = CVLCroppedDataset(NUM_CLASSES, transformation_pipeline=transformation_pipeline, root_dir=DATA_DIR)\n",
    "cvl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278fd2d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Setup the experiment (reset seeds, load datasets, create model, criterion, optimizer and scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad85ad4-8b03-4ebe-a63c-bf37947cf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99521477",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf0342",
   "metadata": {},
   "source": [
    "Load the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c17f2-5c74-47ed-b1cb-f4fe5ed28f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.ImageFolder(os.path.join(DATA_DIR, \"preprocessed\", PREPROCESSED_FILES_DIRNAME, \"train\"),\n",
    "                                           transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                           transforms.RandomRotation(degrees=(-25, 25),fill=1)]))\n",
    "\n",
    "train_set_loader = DataLoader(dataset=train_set, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "                              worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "\n",
    "val_set = datasets.ImageFolder(os.path.join(DATA_DIR, \"preprocessed\", PREPROCESSED_FILES_DIRNAME, \"val\"),\n",
    "                                           transform=transforms.ToTensor())\n",
    "\n",
    "val_set_loader = DataLoader(dataset=val_set, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958d67c",
   "metadata": {},
   "source": [
    "Create model, criterion, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5496fae-8d00-4f1b-98b4-c4c8c3ae79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  vit_lite_7_4(NUM_CLASSES).to(device=device)\n",
    "criterion = LabelSmoothingCrossEntropy().to(device=device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE,\n",
    "                                  weight_decay=WEIGHT_DECAY)\n",
    "scheduler = WarmUpLR(optimizer, LEARNING_RATE, num_epochs_warm_up=NUM_EPOCHS_WARMUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37230cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, criterion, optimizer, scheduler, NUM_EPOCHS, train_set_loader, val_set_loader, \n",
    "                  experiment_name=EXPERIMENT_NAME,\n",
    "                  hyper_params={\n",
    "                                 \"optimizer\": \"adamw\", \n",
    "                                 \"weight decay\": WEIGHT_DECAY, \n",
    "                                 \"lr\": LEARNING_RATE,\n",
    "                                 \"num_epochs_warmup\": NUM_EPOCHS_WARMUP,\n",
    "                                 \"batch_size\": BATCH_SIZE\n",
    "                               },\n",
    "                  log_dir=LOG_DIR,\n",
    "                  saved_models_dir=SAVED_MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c51586",
   "metadata": {},
   "source": [
    "## Train\n",
    "Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0281900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4fe4c",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We evaluate our trained model for writer identification and writer retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(\"saved_models\", EXPERIMENT_NAME, \"epoch_33.pth\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1251d27",
   "metadata": {},
   "source": [
    "### Classification-based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = os.path.join(os.pardir, \"data\", \"preprocessed\", PREPROCESSED_FILES_DIRNAME, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifcation_tester = ClassificationTester(test_set_path, model)\n",
    "\n",
    "classification_results = classifcation_tester(device, 1, NUM_WORKERS, top_k=[1, 2, 3, 5, 10])\n",
    "\n",
    "print(f\"{classification_results=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76aa963",
   "metadata": {},
   "source": [
    "### Retrieval-based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a838c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_tester = RetrievalTester((DIM_SECOND_LAST_LAYER, NUM_CLASSES),\n",
    "                                           test_set_path, model)\n",
    "\n",
    "retrieval_results = retrieval_tester(device, 1, NUM_WORKERS, soft_top_k=[1, 2, 3, 5, 10],\n",
    "                               hard_top_k=[1], metrics=[\"canberra\", \"chebyshev\", \"cityblock\", \"correlation\", \n",
    "                                                        \"cosine\", \"euclidean\", \"seuclidean\", \"sqeuclidean\"])\n",
    "\n",
    "\n",
    "print(f\"{retrieval_results=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05edfa5a",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi, ‘Escaping the Big Data Paradigm with\n",
    "Compact Transformers’, arXiv:2104.05704 [cs], Jun. 2021, Accessed: 2021-07-19. [Online]. Available:\n",
    "http://arxiv.org/abs/2104.05704\n",
    "\n",
    "[2] https://github.com/SHI-Labs/Compact-Transformers/,\n",
    "Accessed: 2021-07-19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}